 <a name="readme-top"></a>

<div align="center">

<a href="" target="_blank" title="Go to  website">
<img width="196px" alt="Document My Project" src="assets/images/icon.png">
</a>

# Movie Recommendation MLOps Project

System of recommendation of movies based on the user's profile and preferences.

</div>
<div align="center"><h4><a href="#-about-the-project">â„¹ï¸ About the Project</a> â€¢ <a href="#-showcase">ğŸ Showcase</a> â€¢ <a href="#-features">â­ï¸ Features</a> â€¢ <a href="#-stack-tech">ğŸ›  Stack Tech</a> â€¢ <a href="#-setup">âš™ ï¸Setup</a> â€¢ <a href="#-from-here-have-fun">ğŸ‰ Have fun!</a> â€¢ <a href="#-about-the-authors">ğŸ‘¨ğŸ»â€ About the Authors</a> â€¢ <a href="#-license">ğŸ“– License</a></h4></div>

## â„¹ï¸ About the Project

This project is a starting pack for MLOps projects focused on the subject of "movie recommendation". It provides a structured framework to develop, train, and deploy machine learning models for recommending movies to users. It uses Supabase for the backend, Airflow for the orchestration, MLflow for the tracking, Minio for the storage of the models, Prometheus and Grafana for the monitoring.


## ğŸ Showcase

<p align="center"><img src="assets/images/dmp_1.png" alt="Main Image"/></p>

<center>
<table>
<tr>
<td><a href="assets/images/dmp_s_1.png"><img width="320" height="200" src="assets/images/dmp_s_1.png"></a></td>
<td><a href="assets/images/dmp_s_2.png"><img width="320" height="200" src="assets/images/dmp_s_2.png"></a></td>
</tr>
<tr>
<td><a href="assets/images/dmp_s_3.jpeg"><img width="320" height="200" src="assets/images/dmp_s_3.jpeg"></a></td>
<td><a href="assets/images/dmp_s_4.png"><img width="320" height="200" src="assets/images/dmp_s_4.png"></a></td>
</tr>
</table>
</center>

## Project Organization

The project is organized as follows:

```
â”œâ”€â”€ .github
â”‚Â Â  â””â”€â”€ workflows
â”‚Â Â      â”œâ”€â”€ test-api.yml                    <- GitHub Actions workflow for testing the API.
â”‚Â Â      â””â”€â”€ build-and-push-images.yml       <- GitHub Actions workflow for building and pushing the images.
â”‚
â”œâ”€â”€ airflow
â”‚Â Â  â”œâ”€â”€ config                           
â”‚Â Â  â”œâ”€â”€ dags
â”‚Â Â  â”‚Â Â  â”œâ”€â”€ scraping_new_movies.py          <- DAG for scraping new movies.
â”‚Â Â  â”‚Â Â  â””â”€â”€ train_model_dag.py              <- DAG for training the model.
â”‚   â”‚
â”‚Â Â  â”œâ”€â”€ logs
â”‚Â Â  â”œâ”€â”€ plugins
â”‚Â Â  â”œâ”€â”€ Dockerfile
â”‚Â Â  â”œâ”€â”€ docker-compose.override.yaml
â”‚Â Â  â”œâ”€â”€ docker-compose.yaml
â”‚Â Â  â””â”€â”€ requirements.txt
â”‚
â”œâ”€â”€ app
â”‚Â Â  â”œâ”€â”€ api
â”‚Â Â  â”‚Â Â  â””â”€â”€ predict
â”‚Â Â  â”‚Â Â      â”œâ”€â”€ Dockerfile
â”‚Â Â  â”‚Â Â      â”œâ”€â”€ main.py                     <- Main file for the API.
â”‚Â Â  â”‚Â Â      â”œâ”€â”€ metrics.py                  <- Metrics for the API.
â”‚Â Â  â”‚Â Â      â””â”€â”€ requirements.txt            <- Requirements for the API.
â”‚   â”‚
â”‚Â Â  â”œâ”€â”€ streamlit
â”‚Â Â  â”‚Â Â  â”œâ”€â”€ pages
â”‚Â Â  â”‚Â Â  â”‚   â”œâ”€â”€ 1_Recommandations.py        <- Page for recommendations.
â”‚Â Â  â”‚Â Â  â”‚   â””â”€â”€ 2_Profil.py                 <- Page for the profile.
â”‚Â Â  â”‚Â Â  â”œâ”€â”€ Dockerfile
â”‚Â Â  â”‚Â Â  â”œâ”€â”€ Home.py                         <- Main page.
â”‚Â Â  â”‚Â Â  â”œâ”€â”€ requirements.txt                <- Requirements for the Streamlit app.
â”‚Â Â  â”‚Â Â  â”œâ”€â”€ style.css                       <- CSS for the pages.
â”‚Â Â  â”‚Â Â  â”œâ”€â”€ supabase_auth.py                <- Supabase authentication.
â”‚Â Â  â”‚Â Â  â””â”€â”€ utils.py                        <- Utility functions.
â”‚   â”‚
â”‚   â””â”€â”€ docker-compose.yml                  <- Docker compose file for the Streamlit app and the API.
â”‚
â”œâ”€â”€ ml
â”‚Â Â  â”œâ”€â”€ models
â”‚Â Â  â”‚Â Â  â””â”€â”€ model.pkl                       <- Initial trained model.
â”‚Â Â  â””â”€â”€ src
â”‚Â Â      â”œâ”€â”€ data
â”‚Â Â      â”‚Â Â  â”œâ”€â”€ check_structure.py          <- Script for checking the structure of the data.
â”‚Â Â      â”‚Â Â  â”œâ”€â”€ import_raw_data.py          <- Script for importing raw data.
â”‚Â Â      â”‚Â Â  â””â”€â”€ load_data_in_db.py          <- Script for loading data into the database.
â”‚Â Â      â”œâ”€â”€ features
â”‚Â Â      â”‚Â Â  â””â”€â”€ build_features.py           <- Script for building features.
â”‚Â Â      â”œâ”€â”€ models
â”‚Â Â      â”‚Â Â  â”œâ”€â”€ predict_model.py            <- Script for making predictions.
â”‚Â Â      â”‚Â Â  â””â”€â”€ train_model.py              <- Script for training the model.
â”‚Â Â      â””â”€â”€ requirements.txt                <- Requirements for the project.
â”‚
â”œâ”€â”€ mlflow
â”‚Â Â  â”œâ”€â”€ docker-compose.yml
â”‚Â Â  â”œâ”€â”€ Dockerfile
â”‚Â Â  â””â”€â”€ requirements.txt
â”‚
â”œâ”€â”€ monitoring
â”‚Â Â  â”œâ”€â”€ grafana
â”‚Â Â  â”‚Â Â  â””â”€â”€ provisioning
â”‚Â Â  â”‚Â Â      â”œâ”€â”€ dashboards
â”‚Â Â  â”‚Â Â      â”‚Â Â  â”œâ”€â”€ api_dashboard.json      <- Dashboard for the API.
â”‚Â Â  â”‚Â Â      â”‚Â Â  â””â”€â”€ dashboards.yml          <- Dashboards configuration.
â”‚Â Â  â”‚Â Â      â””â”€â”€ datasources
â”‚Â Â  â”‚Â Â          â””â”€â”€ datasource.yml          <- Datasource configuration.
â”‚Â Â  â””â”€â”€ prometheus
â”‚Â Â  â”‚   â””â”€â”€ prometheus.yml                  <- Prometheus configuration.
â”‚   â”‚
â”‚Â Â  â””â”€â”€ docker-compose.yml
â”‚
â”œâ”€â”€ supabase
â”‚Â Â  â”œâ”€â”€ README.md
â”‚Â Â  â”œâ”€â”€ docker-compose.override.yml
â”‚Â Â  â”œâ”€â”€ docker-compose.s3.yml
â”‚Â Â  â”œâ”€â”€ docker-compose.yml
â”‚Â Â  â””â”€â”€ volumes
â”‚Â Â      â”œâ”€â”€ api
â”‚Â Â      â”‚Â Â  â””â”€â”€ kong.yml
â”‚Â Â      â”œâ”€â”€ db
â”‚Â Â      â”‚Â Â  â”œâ”€â”€ _supabase.sql
â”‚Â Â      â”‚Â Â  â”œâ”€â”€ init
â”‚Â Â      â”‚Â Â  â”‚Â Â  â”œâ”€â”€ 01-project-tables.sql    <- SQL script for creating project tables.
â”‚Â Â      â”‚Â Â  â”‚Â Â  â”œâ”€â”€ 02-auth-trigger.sql      <- SQL script for creating the auth trigger.
â”‚Â Â      â”‚Â Â  â”‚Â Â  â”œâ”€â”€ 03-security-policies.sql <- SQL script for creating security policies.
â”‚Â Â      â”‚Â Â  â”‚Â Â  â””â”€â”€ data.sql
â”‚Â Â      â”‚Â Â  â”œâ”€â”€ jwt.sql
â”‚Â Â      â”‚Â Â  â”œâ”€â”€ logs.sql
â”‚Â Â      â”‚Â Â  â”œâ”€â”€ pooler.sql
â”‚Â Â      â”‚Â Â  â”œâ”€â”€ realtime.sql
â”‚Â Â      â”‚Â Â  â”œâ”€â”€ roles.sql
â”‚Â Â      â”‚Â Â  â””â”€â”€ webhooks.sql
â”‚Â Â      â”œâ”€â”€ functions
â”‚Â Â      â”‚Â Â  â”œâ”€â”€ hello
â”‚Â Â      â”‚Â Â  â”‚Â Â  â””â”€â”€ index.ts
â”‚Â Â      â”‚Â Â  â””â”€â”€ main
â”‚Â Â      â”‚Â Â      â””â”€â”€ index.ts
â”‚Â Â      â”œâ”€â”€ logs
â”‚Â Â      â”‚Â Â  â””â”€â”€ vector.yml
â”‚Â Â      â””â”€â”€ pooler
â”‚Â Â          â””â”€â”€ pooler.exs
â”‚
â”œâ”€â”€ tests
â”‚   â”œâ”€â”€ requirements.txt                    <- Requirements for the tests.
â”‚   â”œâ”€â”€ test_api_predict.py                 <- Test for the API.
â”‚   â””â”€â”€ test_rls.py                         <- Test for the RLS.
â”‚
â”œâ”€â”€ .env.example                            <- Example of the .env file.
â”œâ”€â”€ .gitignore                              <- Git ignore file.
â”œâ”€â”€ LICENSE
â”œâ”€â”€ Makefile                                <- Makefile for the project.
â”œâ”€â”€ README.md                               <- This README file.
â”œâ”€â”€ requirements-dev.txt
â””â”€â”€ requirements-ref.txt
```

## â­ï¸ Features

- Very basic model to recommend movies.
- Scraping new movies from The Movie Database (TMDB) API.
- Training a machine learning model to recommend movies.
- Deploying the model as a REST API.
- Building an interactive web app to display recommendations.
- Orchestrating the workflow with Apache Airflow.
- Monitoring the system with Prometheus and Grafana.

## ğŸ›  Stack Tech

- **Python**: The main programming language used for data processing, model training, and prediction.
- **Docker & Docker Compose**: Used for containerizing the application and setting up a local development environment.
- **Supabase**: A backend service for managing the database and authentication.
- **MLflow**: For tracking experiments and managing machine learning models.
- **Apache Airflow**: For orchestrating data workflows and model training pipelines.
- **Streamlit**: For building interactive web applications to display recommendations.
- **FastAPI**: For building the REST API for the movie recommendation service.
- **Prometheus**: For monitoring and alerting.
- **Grafana**: For visualizing metrics.

## âš™ ï¸Setup

Make sure you have the following tools installed:
Docker, Docker Compose, Python 3.10+, pip, git et make
   ```bash
   docker --version
   docker compose version
   python --version
   pip --version
   git --version
   make --version
   ```

To set up the project for local development, from the root of the repository follow the steps below:

1. Run the `make setup1` command.

2. Set the environment variable TMDB_API_TOKEN in the .env file. This is necessary to be able to execute the DAG `scraping_new_movies.py` in Airflow. You can get a token [here](https://www.themoviedb.org/settings/api).

3. Run the `make setup2` command.

4. Run the `make start` command.

5. Setup access and the bucket for MLFlow:
   - Access MinIO console at `localhost:9001` and sign in with root credentials from `.env`
   - Create access key and save the generated keys
   - Create a bucket named `mlflow`
   - Update `.env` with your access/secret keys and restart the containers:
     ```bash
     cd mlflow
     docker compose down
     docker compose --env-file ../.env up -d --build
     ```

6. Secrets for the GitHub Actions workflow:
   - In order to push the images to the Docker Hub registry, you need to create a personal access token with the necessary permissions.
   - Add the token (read and write) for the Docker Hub registry as a secret with the name `DOCKERHUB_TOKEN`.
   - Add the username for the Docker Hub registry as a secret with the name `DOCKERHUB_USERNAME`.


**Local access to the Services**:
- Supabase: [http://localhost:8000](http://localhost:8000)
- Airflow: [http://localhost:8080](http://localhost:8080)
- Streamlit: [http://localhost:8501](http://localhost:8501)
- API: [http://localhost:8002/docs](http://localhost:8002/docs)
- MLFlow: [http://localhost:5001](http://localhost:5001)
- MinIO: [http://localhost:9001](http://localhost:9001)
- Prometheus: [http://localhost:9090](http://localhost:9090)
- Grafana: [http://localhost:3000](http://localhost:3000)

## ğŸ‰ From here have fun!

#### First recommendations
You can open the Streamlit app, create an account and have your first recommendations!

In order for the app to work you need to use an email with a number just before the @ symbol (ex: `user1@example.com`, `user20@example.com`, etc.). It'll allow you to have an existing profil linked to your account.

#### Run the DAGs
You can run the DAGs in Airflow to scrape new movies and train the model.

#### Analyse the model in MLFlow
You can explore the artefacts, metrics and logs of the model in MLFlow if you runned the training DAG.

#### Explore Prometheus and Grafana
You can explore the metrics in Grafana's dashboard.

#### Explore Supabase
You can explore the database in Supabase's dashboard.

## ğŸ‘¨ğŸ»â€ About the Authors

**Sarah Hemmel**

**Mikhael Benilouz**

**Antoine Pelamourgues**

## ğŸ“– License

This project is licensed under the [MIT License](https://opensource.org/licenses/MIT).


<p align="right"><a href="#readme-top">Top â¬†ï¸</a></p>

---
 <div align="center">Built with â¤ï¸</div>
