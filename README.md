# Movie Recommendation MLOps Project

This project is a starting pack for MLOps projects focused on the subject of "movie recommendation". It provides a structured framework to develop, train, and deploy machine learning models for recommending movies to users. It uses Supabase for the backend, Airflow for the orchestration, MLflow for the tracking, Minio for the storage of the models, Prometheus and Grafana for the monitoring.

## Project Organization

The project is organized as follows:

```
â”œâ”€â”€ .github
â”‚Â Â  â””â”€â”€ workflows
â”‚Â Â      â””â”€â”€ test-api.yml                    <- GitHub Actions workflow for testing the API.
â”‚
â”œâ”€â”€ airflow
â”‚Â Â  â”œâ”€â”€ Dockerfile
â”‚Â Â  â”œâ”€â”€ dags
â”‚Â Â  â”‚Â Â  â”œâ”€â”€ scraping_new_movies.py          <- DAG for scraping new movies.
â”‚Â Â  â”‚Â Â  â””â”€â”€ train_model_dag.py              <- DAG for training the model.
â”‚Â Â  â”œâ”€â”€ docker-compose.override.yaml
â”‚Â Â  â”œâ”€â”€ docker-compose.yaml
â”‚Â Â  â”œâ”€â”€ logs
â”‚Â Â  â””â”€â”€ requirements.txt
â”‚
â”œâ”€â”€ api
â”‚Â Â  â””â”€â”€ predict
â”‚Â Â      â”œâ”€â”€ Dockerfile
â”‚Â Â      â”œâ”€â”€ main.py                         <- Main file for the API.
â”‚Â Â      â”œâ”€â”€ metrics.py                      <- Metrics for the API.
â”‚Â Â      â””â”€â”€ requirements.txt                <- Requirements for the API.
â”‚
â”œâ”€â”€ ml
â”‚Â Â  â”œâ”€â”€ models
â”‚Â Â  â”‚Â Â  â””â”€â”€ model.pkl                       <- Initial trained model.
â”‚Â Â  â””â”€â”€ src
â”‚Â Â      â”œâ”€â”€ data
â”‚Â Â      â”‚Â Â  â”œâ”€â”€ check_structure.py          <- Script for checking the structure of the data.
â”‚Â Â      â”‚Â Â  â”œâ”€â”€ import_raw_data.py          <- Script for importing raw data.
â”‚Â Â      â”‚Â Â  â””â”€â”€ load_data_in_db.py          <- Script for loading data into the database.
â”‚Â Â      â”œâ”€â”€ features
â”‚Â Â      â”‚Â Â  â””â”€â”€ build_features.py           <- Script for building features.
â”‚Â Â      â”œâ”€â”€ models
â”‚Â Â      â”‚Â Â  â”œâ”€â”€ predict_model.py            <- Script for making predictions.
â”‚Â Â      â”‚Â Â  â””â”€â”€ train_model.py              <- Script for training the model.
â”‚Â Â      â””â”€â”€ requirements.txt                <- Requirements for the project.
â”‚
â”œâ”€â”€ mlflow
â”‚Â Â  â”œâ”€â”€ Dockerfile
â”‚Â Â  â””â”€â”€ requirements.txt
â”‚
â”œâ”€â”€ monitoring
â”‚Â Â  â”œâ”€â”€ grafana
â”‚Â Â  â”‚Â Â  â””â”€â”€ provisioning
â”‚Â Â  â”‚Â Â      â”œâ”€â”€ dashboards
â”‚Â Â  â”‚Â Â      â”‚Â Â  â”œâ”€â”€ api_dashboard.json      <- Dashboard for the API.
â”‚Â Â  â”‚Â Â      â”‚Â Â  â””â”€â”€ dashboards.yml          <- Dashboards configuration.
â”‚Â Â  â”‚Â Â      â””â”€â”€ datasources
â”‚Â Â  â”‚Â Â          â””â”€â”€ datasource.yml          <- Datasource configuration.
â”‚Â Â  â””â”€â”€ prometheus
â”‚Â Â      â””â”€â”€ prometheus.yml                  <- Prometheus configuration.
â”‚
â”œâ”€â”€ streamlit
â”‚Â Â  â””â”€â”€ pages
â”‚Â Â  â”‚   â”œâ”€â”€ 1_Recommandations.py            <- Page for recommendations.
â”‚Â Â  â”‚   â””â”€â”€ 2_Profil.py                     <- Page for the profile.
â”‚Â Â  â”œâ”€â”€ Dockerfile
â”‚Â Â  â”œâ”€â”€ Home.py                             <- Main page.
â”‚Â Â  â”œâ”€â”€ requirements.txt                    <- Requirements for the Streamlit app.
â”‚Â Â  â”œâ”€â”€ style.css                           <- CSS for the pages.
â”‚Â Â  â”œâ”€â”€ supabase_auth.py                    <- Supabase authentication.
â”‚Â Â  â””â”€â”€ utils.py                            <- Utility functions.
â”‚

â”œâ”€â”€ supabase
â”‚Â Â  â”œâ”€â”€ README.md
â”‚Â Â  â”œâ”€â”€ docker-compose.override.yml
â”‚Â Â  â”œâ”€â”€ docker-compose.s3.yml
â”‚Â Â  â”œâ”€â”€ docker-compose.yml
â”‚Â Â  â””â”€â”€ volumes
â”‚Â Â      â”œâ”€â”€ api
â”‚Â Â      â”‚Â Â  â””â”€â”€ kong.yml
â”‚Â Â      â”œâ”€â”€ db
â”‚Â Â      â”‚Â Â  â”œâ”€â”€ _supabase.sql
â”‚Â Â      â”‚Â Â  â”œâ”€â”€ init
â”‚Â Â      â”‚Â Â  â”‚Â Â  â”œâ”€â”€ 01-project-tables.sql    <- SQL script for creating project tables.
â”‚Â Â      â”‚Â Â  â”‚Â Â  â”œâ”€â”€ 02-auth-trigger.sql      <- SQL script for creating the auth trigger.
â”‚Â Â      â”‚Â Â  â”‚Â Â  â”œâ”€â”€ 03-security-policies.sql <- SQL script for creating security policies.
â”‚Â Â      â”‚Â Â  â”‚Â Â  â””â”€â”€ data.sql
â”‚Â Â      â”‚Â Â  â”œâ”€â”€ jwt.sql
â”‚Â Â      â”‚Â Â  â”œâ”€â”€ logs.sql
â”‚Â Â      â”‚Â Â  â”œâ”€â”€ pooler.sql
â”‚Â Â      â”‚Â Â  â”œâ”€â”€ realtime.sql
â”‚Â Â      â”‚Â Â  â”œâ”€â”€ roles.sql
â”‚Â Â      â”‚Â Â  â””â”€â”€ webhooks.sql
â”‚Â Â      â”œâ”€â”€ functions
â”‚Â Â      â”‚Â Â  â”œâ”€â”€ hello
â”‚Â Â      â”‚Â Â  â”‚Â Â  â””â”€â”€ index.ts
â”‚Â Â      â”‚Â Â  â””â”€â”€ main
â”‚Â Â      â”‚Â Â      â””â”€â”€ index.ts
â”‚Â Â      â”œâ”€â”€ logs
â”‚Â Â      â”‚Â Â  â””â”€â”€ vector.yml
â”‚Â Â      â””â”€â”€ pooler
â”‚Â Â          â””â”€â”€ pooler.exs
â”‚
â”œâ”€â”€ tests
â”‚   â”œâ”€â”€ requirements.txt                    <- Requirements for the tests.
â”‚   â”œâ”€â”€ test_api_predict.py                 <- Test for the API.
â”‚   â””â”€â”€ test_rls.py                         <- Test for the RLS.
â”‚
â”œâ”€â”€ .env.example                            <- Example of the .env file.
â”œâ”€â”€ .gitignore                              <- Git ignore file.
â”œâ”€â”€ docker-compose.yml                      <- Docker compose file.
â”œâ”€â”€ LICENSE
â”œâ”€â”€ Makefile                                <- Makefile for the project.
â”œâ”€â”€ README.md                               <- This README file.
â”œâ”€â”€ requirements-dev.txt
â””â”€â”€ requirements-ref.txt
```

## Tools Used

- **Python**: The main programming language used for data processing, model training, and prediction.
- **Docker & Docker Compose**: Used for containerizing the application and setting up a local development environment.
- **Supabase**: A backend service for managing the database and authentication.
- **MLflow**: For tracking experiments and managing machine learning models.
- **Apache Airflow**: For orchestrating data workflows and model training pipelines.
- **Streamlit**: For building interactive web applications to display recommendations.
- **FastAPI**: For building the REST API for the movie recommendation service.
- **Prometheus**: For monitoring and alerting.
- **Grafana**: For visualizing metrics.

## Setting Up for Local Development

To set up the project for local development, follow these steps (every command is as you would execute it in the root of the repository):

1. **Clone the Repository**:
   ```bash
   git clone <repository-url>
   cd <repository-directory>
   ```

2. **Create a Virtual Environment**:
   ```bash
   python -m venv .venv
   ```

3. **Activate the Virtual Environment**:
   - On Windows:
     ```bash
     .\.venv\Scripts\activate
     ```
   - On macOS/Linux:
     ```bash
     source .venv/bin/activate
     ```

4. **Install Required Packages**:
   ```bash
   pip install -r requirements-dev.txt
   ```

5. **Run the Application**:
   - Execute the data import and processing scripts:
     ```bash
     python ml/src/data/import_raw_data.py
     python ml/src/features/build_features.py
     ```

6. **Train the Model**:
   ```bash
   python ml/src/models/train_model.py
   ```

7. **Set the Environment Variables**:
   - Copy the .env.example file to .env and set the desired variables.
   - In the airflow folder, copy the .env.example file to .env and set the desired variables. The AIRFLOW_UID variable is set like this:
     ```bash
     AIRFLOW_UID=$(id -u) >> .env
     ```
   - In the supabase folder, copy the .env.example file to .env and set the desired variables.

> ðŸ’¡ **Alternatively**: For a simplified setup, you can run the `make setup1` command which will automatically execute steps 1 through 7. Then set the environment variables as explained above.

The following steps are executed in the root of the repository and require the environment variables to be set in .env files (see step 7).

8. **Create the Docker network**:
   ```bash
   docker network create backend
   ```

9. **Build and Up the Supabase services and load the data in the database**:
   - Pull Supabase images:
     ```bash
     cd supabase && docker compose pull
     ```
   - Up Supabase services:
     ```bash
     cd supabase && docker compose up -d
     ```
   - Load data in the database (make sure the database is ready, it may take a minute):
     ```bash
     python ml/src/data/load_data_in_db.py
     ``` 

10. **Build and Up the Airflow services**:
   - Initialize Airflow:
     ```bash
     cd airflow && docker compose up airflow-init
     ```
   - Up Airflow services:
     ```bash
     cd airflow && docker compose up -d
     ```

11. **Build and Up the other services (API, Streamlit, MLflow + Minio + db, Prometheus, Grafana)**:
   - Build the services:
     ```bash
     docker compose build
     ```
   - Up the services:
     ```bash
     docker compose up -d
     ```

> ðŸ’¡ **Alternatively**: For a simplified setup, you can run the `make setup2` command followed by the `make start` command which will automatically execute steps 8 through 11.


**Access the Services**:
- Supabase: [http://localhost:8000](http://localhost:8000)
- Airflow: [http://localhost:8080](http://localhost:8080)
- Streamlit: [http://localhost:8501](http://localhost:8501)
- MLflow: [http://localhost:5001](http://localhost:5001)
- Minio: [http://localhost:9001](http://localhost:9001)
- Prometheus: [http://localhost:9090](http://localhost:9090)
- Grafana: [http://localhost:3000](http://localhost:3000)


## TODO
- [ ] parler du docker compose override
- [ ] parler de github actions
- [ ] parler des tests
- [ ] parler du endpoint /reload_model
